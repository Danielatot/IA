{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8780e802f938c97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T13:34:45.381084Z",
     "start_time": "2025-11-09T13:34:45.379395Z"
    }
   },
   "source": [
    "## Importing the libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "778a637b838440c",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from typing import Union, List, Dict\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import re\n",
    "import xlsxwriter\n",
    "\n",
    "# Import statements for Functions.py\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from src.python.Functions import describe_dataframes\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..', 'src', 'python')))\n",
    "from Functions import save_df_list_to_csv_auto, describe_dataframes\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b622c9ade013314",
   "metadata": {},
   "source": [
    "## Importing the datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f20bf31bc77de70f",
   "metadata": {},
   "source": [
    "# Importing dataset of balance and P&L function:\n",
    "# Execution might take 10 minutes or more due to the large size of the datasets.\n",
    "# Cannot make pandas read csv without errors so we use openpyxl to read xlsx files.\n",
    "\n",
    "balance2025 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2025.xlsx')\n",
    "balance2024 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2024.xlsx')\n",
    "balance2023 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2023.xlsx')\n",
    "balance2022 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2022.xlsx')\n",
    "balance2021 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2021.xlsx')\n",
    "balance2020 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2020.xlsx')\n",
    "\n",
    "pnl2025 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2025.xlsx')\n",
    "pnl2024 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2024.xlsx')\n",
    "pnl2023 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2023.xlsx')\n",
    "pnl2022 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2022.xlsx')\n",
    "pnl2021 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2021.xlsx')\n",
    "pnl2020 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2020.xlsx')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9863e5bf1b1e5f22",
   "metadata": {},
   "source": [
    "### Fallbacks for large dfs:\n",
    "Run from here when you make changes to the dataframes to avoid memory errors and long file importing!"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4f6153048a8682",
   "metadata": {},
   "source": [
    "\n",
    "# Fallbacks for large dfs:\n",
    "B2025 = balance2025.copy()\n",
    "B2024 = balance2024.copy()\n",
    "B2023 = balance2023.copy()\n",
    "B2022 = balance2022.copy()\n",
    "B2021 = balance2021.copy()\n",
    "B2020 = balance2020.copy()\n",
    "\n",
    "P2025 = pnl2025.copy()\n",
    "P2024 = pnl2024.copy()\n",
    "P2023 = pnl2023.copy()\n",
    "P2022 = pnl2022.copy()\n",
    "P2021 = pnl2021.copy()\n",
    "P2020 = pnl2020.copy()\n",
    "\n",
    "# Saving in lists for functions:\n",
    "balance_list = [B2025, B2024, B2023, B2022, B2021, B2020]\n",
    "pnl_list = [P2025, P2024, P2023, P2022, P2021, P2020]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f27eec3673db73cd",
   "metadata": {},
   "source": [
    "## Cleaning the data with functions:\n",
    "#### Removing unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6d2875c38ddb353",
   "metadata": {},
   "source": [
    "# Unnecessary column removal from list of dataframes:\n",
    "def remove_mutual_unnecessary_columns(df_list):\n",
    "    for df in df_list:\n",
    "        bad_columns = ['ja_pavadinimas', 'obj_pav','form_pav','template_name',  'standard_name', 'form_pavadinimas','line_type_id', 'stat_pavadinimas', 'stat_pav']\n",
    "        for col in bad_columns:\n",
    "            if col in df.columns:\n",
    "                df.drop(columns=col, inplace=True)\n",
    "    return df_list\n",
    "\n",
    "# Removing unnecessary rows from list of dataframes:\n",
    "remove_mutual_unnecessary_columns(balance_list)\n",
    "remove_mutual_unnecessary_columns(pnl_list)\n",
    "\n",
    "# Renaming collumns to be the same across dataframes:\n",
    "def rename_columns(df_list):\n",
    "    for df in df_list:\n",
    "        # Rename 'obj_kodas' to 'ja_kodas' if it exists\n",
    "        if 'obj_kodas' in df.columns:\n",
    "            df.rename(columns={'obj_kodas': 'ja_kodas'}, inplace=True)\n",
    "\n",
    "        # Rename other columns if they exist\n",
    "        column_mapping = {\n",
    "            'nuosavas_kapitalas': 'NUOSAVAS KAPITALAS',\n",
    "            'mok_sumos_ir_isipareigojimai': 'MOKĖTINOS SUMOS IR KITI ĮSIPAREIGOJIMAI',\n",
    "            'trumpalaikis_turtas': 'TRUMPALAIKIS TURTAS',\n",
    "            'ilgalaikis_turtas': 'ILGALAIKIS TURTAS',\n",
    "            'pelnas_pries_apmokestinima': 'PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ',\n",
    "        'grynasis_pelnas': 'GRYNASIS PELNAS (NUOSTOLIAI)',\n",
    "        'pardavimo_pajamos': 'PARDAVIMO PAJAMOS'\n",
    "        }\n",
    "\n",
    "        for old_col, new_col in column_mapping.items():\n",
    "            if old_col in df.columns:\n",
    "                df.rename(columns={old_col: new_col}, inplace=True)\n",
    "\n",
    "    return df_list\n",
    "\n",
    "rename_columns(balance_list)\n",
    "rename_columns(pnl_list)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95312fa08ed7ae2e",
   "metadata": {},
   "source": [
    "#### Extracting columns line_name, reiksme and ja_kodas:\n",
    "This function is uneeded but if you need to view just the extracted columns, you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d99a3f07c2c9d54",
   "metadata": {},
   "source": [
    "# Extracting columns line_name, reiksme and ja_kodas from dfs with all of this data:\n",
    "def extract_line_name_reiksme_ja_kodas(df_list):\n",
    "    \"\"\"\n",
    "    Extract columns 'line_name', 'reiksme', and 'ja_kodas' from DataFrames\n",
    "    that contain all three columns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of pandas.DataFrame\n",
    "        List of DataFrames to process (will be modified in-place)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of pandas.DataFrame\n",
    "        List of DataFrames containing only the three specified columns\n",
    "    \"\"\"\n",
    "    extracted_dfs = []\n",
    "\n",
    "    required_columns = ['line_name', 'reiksme', 'ja_kodas']\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        # Check if all required columns exist in the current DataFrame\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            # Extract only the required columns\n",
    "            extracted_df = df[required_columns].copy()\n",
    "            extracted_dfs.append(extracted_df)\n",
    "\n",
    "\n",
    "        else:\n",
    "            missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "            print(f\"DataFrame {i}: Missing columns {missing_cols} - skipped\")\n",
    "\n",
    "    print(f\"\\nExtracted {len(extracted_dfs)} out of {len(df_list)} DataFrames\")\n",
    "    return extracted_dfs\n",
    "\"\"\" Uncomment to see the extracted columns:\n",
    "B_extracted = extract_line_name_reiksme_ja_kodas(balance_list)\n",
    "P_extracted = extract_line_name_reiksme_ja_kodas(pnl_list)\n",
    "\n",
    "# Renaming the extracted dfs to be more descriptive:\n",
    "B_extracted_2025 = B_extracted[0]\n",
    "B_extracted_2024 = B_extracted[1]\n",
    "\n",
    "P_extracted_2025 = P_extracted[0]\n",
    "P_extracted_2024 = P_extracted[1]\n",
    "\n",
    "# New lists with extracted and renamed dfs:\n",
    "B_extracted_renamed = [B_extracted_2025, B_extracted_2024]\n",
    "P_extracted_renamed = [P_extracted_2025, P_extracted_2024]\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Shifting the column data of extraced dfs so that every row is unique with new columns:",
   "id": "953c605149cc459d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pivoting the big dataframes:\n",
    "def pivot_dfs_smart(df_list):\n",
    "    \"\"\"\n",
    "    Apply pivot transformation with smart aggregation for different column types.\n",
    "    \"\"\"\n",
    "\n",
    "    def pivot_line_names_to_columns_smart(df):\n",
    "        \"\"\"\n",
    "        Pivot with intelligent aggregation based on column data types.\n",
    "        \"\"\"\n",
    "        # Identify column types for smart aggregation\n",
    "        numeric_columns = []\n",
    "        string_columns = []\n",
    "\n",
    "        for col in df.columns:\n",
    "            if col in ['line_name', 'reiksme']:\n",
    "                continue\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                numeric_columns.append(col)\n",
    "            else:\n",
    "                string_columns.append(col)\n",
    "\n",
    "        # Create aggregation dictionary\n",
    "        aggregation_dict = {'reiksme': 'first'}\n",
    "\n",
    "        # For numeric columns, use 'first' or 'mean' depending on context\n",
    "        for col in numeric_columns:\n",
    "            aggregation_dict[col] = 'first'\n",
    "\n",
    "        # For string columns, use 'first' (take the first occurrence)\n",
    "        for col in string_columns:\n",
    "            aggregation_dict[col] = 'first'\n",
    "\n",
    "        # Identify index columns (all columns except line_name and reiksme)\n",
    "        index_columns = [col for col in df.columns if col not in ['line_name', 'reiksme']]\n",
    "\n",
    "        # Perform pivot\n",
    "        pivoted_df = df.pivot_table(\n",
    "            index=index_columns,\n",
    "            columns='line_name',\n",
    "            values='reiksme',\n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "\n",
    "        # Reset column names\n",
    "        pivoted_df.columns.name = None\n",
    "\n",
    "        return pivoted_df\n",
    "\n",
    "    pivoted_dfs = []\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        try:\n",
    "            required_columns = ['line_name', 'reiksme', 'ja_kodas']\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                print(f\"DataFrame {i}: Missing columns {missing_cols}\")\n",
    "                pivoted_dfs.append(df)\n",
    "                continue\n",
    "\n",
    "            print(f\"DataFrame {i}: Preserving {len(df.columns) - 2} columns in result\")\n",
    "\n",
    "            pivoted_df = pivot_line_names_to_columns_smart(df)\n",
    "            pivoted_dfs.append(pivoted_df)\n",
    "            print(f\"DataFrame {i}: Successfully pivoted. Original shape: {df.shape}, Pivoted shape: {pivoted_df.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"DataFrame {i}: Error during pivoting - {e}\")\n",
    "            pivoted_dfs.append(df)\n",
    "\n",
    "    return pivoted_dfs\n",
    "\n",
    "# Pivoting the balance dataframes:\n",
    "\n",
    "Bpivoted = pivot_dfs_smart(balance_list)\n",
    "\n",
    "# Pivoting the P&L dataframes:\n",
    "\n",
    "Ppivoted = pivot_dfs_smart(pnl_list)\n",
    "\n"
   ],
   "id": "1e0a0b180a91594e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Checking duplicate rows in all dfs:\n",
    "\n",
    "This removes duplicate rows from all dfs and keeps only the most recent row for each ja_kodas, it keeps the new revision data. Older revisions are extracted and saved in a separate df."
   ],
   "id": "af74efbfac6a8d94"
  },
  {
   "cell_type": "code",
   "id": "4d5a00fcbbfcad10",
   "metadata": {},
   "source": [
    "# Checking duplicate rows in all dfs and removing/extracting them:\n",
    "\n",
    "def extract_and_remove_older_duplicates(df_list, date_column='reg_date', id_column='ja_kodas'):\n",
    "    \"\"\"\n",
    "    Find duplicate ja_kodas rows, keep the most recent reg_date, and extract older duplicates.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of pandas.DataFrame\n",
    "        List of DataFrames to process\n",
    "    date_column : str, default 'reg_date'\n",
    "        Name of the date column to determine which rows to keep\n",
    "    id_column : str, default 'ja_kodas'\n",
    "        Name of the ID column to check for duplicates\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (cleaned_dfs, extracted_dfs, summary)\n",
    "        cleaned_dfs: List of DataFrames with only most recent rows kept\n",
    "        extracted_dfs: List of DataFrames containing the older duplicate rows\n",
    "        summary: Dictionary with statistics for each DataFrame\n",
    "    \"\"\"\n",
    "    cleaned_dfs = []\n",
    "    extracted_dfs = []\n",
    "    summary = {}\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        try:\n",
    "            # Check if required columns exist\n",
    "            if id_column not in df.columns:\n",
    "                print(f\"DataFrame {i}: '{id_column}' column not found. Available: {list(df.columns)}\")\n",
    "                cleaned_dfs.append(df)\n",
    "                extracted_dfs.append(pd.DataFrame())  # Empty DataFrame for consistency\n",
    "                summary[i] = {'total_rows': len(df), 'duplicates_found': 0, 'kept': len(df), 'extracted': 0}\n",
    "                continue\n",
    "\n",
    "            if date_column not in df.columns:\n",
    "                print(f\"DataFrame {i}: '{date_column}' column not found. Available: {list(df.columns)}\")\n",
    "                cleaned_dfs.append(df)\n",
    "                extracted_dfs.append(pd.DataFrame())\n",
    "                summary[i] = {'total_rows': len(df), 'duplicates_found': 0, 'kept': len(df), 'extracted': 0}\n",
    "                continue\n",
    "\n",
    "            # Make a copy to avoid modifying original\n",
    "            df_working = df.copy()\n",
    "\n",
    "            # Ensure date_column is datetime\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df_working[date_column]):\n",
    "                df_working[date_column] = pd.to_datetime(df_working[date_column], errors='coerce')\n",
    "                # Remove rows where date conversion failed\n",
    "                invalid_dates = df_working[date_column].isna().sum()\n",
    "                if invalid_dates > 0:\n",
    "                    print(f\"DataFrame {i}: {invalid_dates} rows with invalid dates will be treated as oldest\")\n",
    "\n",
    "            # Find duplicate ja_kodas\n",
    "            duplicate_mask = df_working.duplicated(subset=[id_column], keep=False)\n",
    "            duplicate_rows = df_working[duplicate_mask]\n",
    "\n",
    "            if len(duplicate_rows) == 0:\n",
    "                print(f\"DataFrame {i}: No duplicate {id_column} found\")\n",
    "                cleaned_dfs.append(df_working)\n",
    "                extracted_dfs.append(pd.DataFrame())\n",
    "                summary[i] = {'total_rows': len(df), 'duplicates_found': 0, 'kept': len(df), 'extracted': 0}\n",
    "                continue\n",
    "\n",
    "            # Group by ja_kodas and find the most recent date for each\n",
    "            duplicate_groups = duplicate_rows.groupby(id_column)\n",
    "\n",
    "            # Identify rows to keep (most recent date) and extract (older dates)\n",
    "            keep_indices = []\n",
    "            extract_indices = []\n",
    "\n",
    "            for ja_kodas, group in duplicate_groups:\n",
    "                if len(group) > 1:\n",
    "                    # Find the row with the most recent date\n",
    "                    most_recent_idx = group[date_column].idxmax()\n",
    "                    keep_indices.append(most_recent_idx)\n",
    "\n",
    "                    # Extract all other rows (older dates)\n",
    "                    older_indices = group.index[group.index != most_recent_idx].tolist()\n",
    "                    extract_indices.extend(older_indices)\n",
    "\n",
    "            # Create DataFrames\n",
    "            keep_df = df_working[~df_working.index.isin(extract_indices)]  # All non-extracted rows\n",
    "            extract_df = df_working[df_working.index.isin(extract_indices)]  # Only extracted rows\n",
    "\n",
    "            # Also include non-duplicate rows in keep_df\n",
    "            non_duplicate_rows = df_working[~duplicate_mask]\n",
    "            final_keep_df = pd.concat([keep_df, non_duplicate_rows], ignore_index=True)\n",
    "\n",
    "            cleaned_dfs.append(final_keep_df)\n",
    "            extracted_dfs.append(extract_df)\n",
    "\n",
    "            # Summary statistics\n",
    "            summary[i] = {\n",
    "                'total_rows': len(df),\n",
    "                'duplicates_found': len(duplicate_rows),\n",
    "                'unique_duplicate_ids': duplicate_rows[id_column].nunique(),\n",
    "                'kept': len(final_keep_df),\n",
    "                'extracted': len(extract_df),\n",
    "                'example_extracted': extract_df[id_column].value_counts().head(3).to_dict() if len(extract_df) > 0 else {}\n",
    "            }\n",
    "\n",
    "            print(f\"DataFrame {i}: Kept {len(final_keep_df)} rows, extracted {len(extract_df)} older duplicates\")\n",
    "            if len(extract_df) > 0:\n",
    "                print(f\"  Extracted {extract_df[id_column].nunique()} unique {id_column}s with older dates\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"DataFrame {i}: Error processing - {e}\")\n",
    "            cleaned_dfs.append(df)\n",
    "            extracted_dfs.append(pd.DataFrame())\n",
    "            summary[i] = {'total_rows': len(df), 'duplicates_found': 0, 'kept': len(df), 'extracted': 0, 'error': str(e)}\n",
    "\n",
    "    # Final summary\n",
    "    total_extracted = sum(s['extracted'] for s in summary.values())\n",
    "    total_duplicates = sum(s['duplicates_found'] for s in summary.values())\n",
    "\n",
    "    print(f\"\\n=== EXTRACTION SUMMARY ===\")\n",
    "    print(f\"Total DataFrames processed: {len(df_list)}\")\n",
    "    print(f\"Total duplicate rows found: {total_duplicates}\")\n",
    "    print(f\"Total older rows extracted: {total_extracted}\")\n",
    "\n",
    "    return cleaned_dfs, extracted_dfs, summary\n",
    "\n",
    "# Balance data:\n",
    "cleanedB, extractedB, summaryB = extract_and_remove_older_duplicates(Bpivoted)\n",
    "\n",
    "# P&L data:\n",
    "cleanedP, extractedP, summaryP = extract_and_remove_older_duplicates(Ppivoted)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f80e539a2a1c4eac",
   "metadata": {},
   "source": [
    "## Adding PnL data to balance data:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5497bebf8372e3c",
   "metadata": {},
   "source": [
    "def merge_pnl_and_balances(df_list1, df_list2, how='inner'):\n",
    "    \"\"\"\n",
    "    Merge two lists of DataFrames on ja_kodas column with comprehensive diagnostics and validation.\n",
    "    Each DataFrame from list1 is merged with corresponding DataFrame from list2.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list1, df_list2 : list of pandas.DataFrame\n",
    "        Lists of DataFrames to merge\n",
    "    how : str, default 'inner'\n",
    "        Type of merge: 'inner', 'left', 'right', 'outer'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of pandas.DataFrame\n",
    "        List of merged DataFrames\n",
    "    \"\"\"\n",
    "    if len(df_list1) != len(df_list2):\n",
    "        print(f\"Warning: List lengths differ - list1: {len(df_list1)}, list2: {len(df_list2)}\")\n",
    "        # Use the minimum length to avoid index errors\n",
    "        min_length = min(len(df_list1), len(df_list2))\n",
    "        df_list1 = df_list1[:min_length]\n",
    "        df_list2 = df_list2[:min_length]\n",
    "\n",
    "    merged_dfs = []\n",
    "\n",
    "    for i, (df1, df2) in enumerate(zip(df_list1, df_list2)):\n",
    "        try:\n",
    "            # Check if ja_kodas exists in both DataFrames\n",
    "            if 'ja_kodas' not in df1.columns:\n",
    "                print(f\"Pair {i}: ja_kodas not found in first DataFrame, skipping\")\n",
    "                continue\n",
    "            if 'ja_kodas' not in df2.columns:\n",
    "                print(f\"Pair {i}: ja_kodas not found in second DataFrame, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Create copies to avoid modifying originals\n",
    "            df1_clean = df1.copy()\n",
    "            df2_clean = df2.copy()\n",
    "\n",
    "            # Validate ja_kodas data types and convert if necessary\n",
    "            if df1_clean['ja_kodas'].dtype != df2_clean['ja_kodas'].dtype:\n",
    "                print(f\"Pair {i}: ja_kodas data types differ - converting both to string\")\n",
    "                df1_clean['ja_kodas'] = df1_clean['ja_kodas'].astype(str)\n",
    "                df2_clean['ja_kodas'] = df2_clean['ja_kodas'].astype(str)\n",
    "\n",
    "            # Check for duplicate ja_kodas within each DataFrame\n",
    "            df1_duplicates = df1_clean.duplicated(subset=['ja_kodas']).sum()\n",
    "            df2_duplicates = df2_clean.duplicated(subset=['ja_kodas']).sum()\n",
    "\n",
    "            if df1_duplicates > 0:\n",
    "                print(f\"Pair {i}: WARNING - {df1_duplicates} duplicate ja_kodas found in first DataFrame\")\n",
    "                # Keep first occurrence of duplicates\n",
    "                df1_clean = df1_clean.drop_duplicates(subset=['ja_kodas'], keep='first')\n",
    "\n",
    "            if df2_duplicates > 0:\n",
    "                print(f\"Pair {i}: WARNING - {df2_duplicates} duplicate ja_kodas found in second DataFrame\")\n",
    "                # Keep first occurrence of duplicates\n",
    "                df2_clean = df2_clean.drop_duplicates(subset=['ja_kodas'], keep='first')\n",
    "\n",
    "            # Check for NaN values in ja_kodas\n",
    "            df1_nan = df1_clean['ja_kodas'].isna().sum()\n",
    "            df2_nan = df2_clean['ja_kodas'].isna().sum()\n",
    "\n",
    "            if df1_nan > 0:\n",
    "                print(f\"Pair {i}: WARNING - {df1_nan} NaN values in ja_kodas (first DataFrame), removing\")\n",
    "                df1_clean = df1_clean.dropna(subset=['ja_kodas'])\n",
    "\n",
    "            if df2_nan > 0:\n",
    "                print(f\"Pair {i}: WARNING - {df2_nan} NaN values in ja_kodas (second DataFrame), removing\")\n",
    "                df2_clean = df2_clean.dropna(subset=['ja_kodas'])\n",
    "\n",
    "            # Pre-merge diagnostics\n",
    "            df1_unique = df1_clean['ja_kodas'].nunique()\n",
    "            df2_unique = df2_clean['ja_kodas'].nunique()\n",
    "            common_ja_kodas = set(df1_clean['ja_kodas']) & set(df2_clean['ja_kodas'])\n",
    "            common_count = len(common_ja_kodas)\n",
    "\n",
    "            print(f\"\\n--- Pair {i} Merge Diagnostics ---\")\n",
    "            print(f\"DF1: {len(df1_clean)} rows, {df1_unique} unique ja_kodas\")\n",
    "            print(f\"DF2: {len(df2_clean)} rows, {df2_unique} unique ja_kodas\")\n",
    "            print(f\"Common ja_kodas: {common_count}\")\n",
    "            print(f\"Merge type: {how}\")\n",
    "\n",
    "            # Calculate expected result sizes\n",
    "            if how == 'inner':\n",
    "                expected_rows = common_count\n",
    "            elif how == 'left':\n",
    "                expected_rows = len(df1_clean)\n",
    "            elif how == 'right':\n",
    "                expected_rows = len(df2_clean)\n",
    "            else:  # outer\n",
    "                expected_rows = len(df1_clean) + len(df2_clean) - common_count\n",
    "\n",
    "            print(f\"Expected result rows: {expected_rows}\")\n",
    "\n",
    "            # Perform the merge with indicator to track sources\n",
    "            merged_df = pd.merge(\n",
    "                df1_clean,\n",
    "                df2_clean,\n",
    "                on='ja_kodas',\n",
    "                how=how,\n",
    "                suffixes=('_pnl', '_balance'),\n",
    "                indicator=True  # Add merge indicator column\n",
    "            )\n",
    "\n",
    "            # Post-merge diagnostics\n",
    "            actual_rows = len(merged_df)\n",
    "            merge_stats = merged_df['_merge'].value_counts()\n",
    "\n",
    "            print(f\"Actual result rows: {actual_rows}\")\n",
    "            print(f\"Merge composition: {merge_stats.to_dict()}\")\n",
    "\n",
    "            if actual_rows != expected_rows:\n",
    "                print(f\"WARNING: Expected {expected_rows} rows but got {actual_rows} rows\")\n",
    "\n",
    "            # Remove the indicator column\n",
    "            merged_df = merged_df.drop('_merge', axis=1)\n",
    "\n",
    "            # Check for overlapping column names (besides ja_kodas)\n",
    "            overlapping_cols = set(df1_clean.columns) & set(df2_clean.columns) - {'ja_kodas'}\n",
    "            if overlapping_cols:\n",
    "                print(f\"Overlapping columns (received suffixes): {list(overlapping_cols)}\")\n",
    "\n",
    "            print(f\"Pair {i}: Successfully merged. Shapes: {df1.shape} + {df2.shape} -> {merged_df.shape}\")\n",
    "\n",
    "            merged_dfs.append(merged_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Pair {i}: Error during merge - {e}\")\n",
    "            print(f\"Pair {i}: DF1 columns: {list(df1.columns) if 'df1' in locals() else 'N/A'}\")\n",
    "            print(f\"Pair {i}: DF2 columns: {list(df2.columns) if 'df2' in locals() else 'N/A'}\")\n",
    "            # Keep both original DataFrames if merge fails\n",
    "            merged_dfs.extend([df1, df2])\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n=== MERGE SUMMARY ===\")\n",
    "    print(f\"Successfully processed: {len(merged_dfs)} DataFrames\")\n",
    "    print(f\"Total input pairs: {min(len(df_list1), len(df_list2))}\")\n",
    "\n",
    "    return merged_dfs\n",
    "\n",
    "# Merge the cleaned balance and P&L data:\n",
    "MergedData = merge_pnl_and_balances(cleanedB, cleanedP)\n",
    "\n",
    "# Merging the cleaned balance and P&L data but with outer merge (all rows are kept):\n",
    "# Seems to be the same as inner merge, I don't know why.\n",
    "MergedDataAll = merge_pnl_and_balances(cleanedB, cleanedP, how='outer')\n",
    "\n",
    "# Renaming dfs in MergedData to be more descriptive:\n",
    "\n",
    "MergedBP2025 = MergedData[0]\n",
    "MergedBP2024 = MergedData[1]\n",
    "MergedBP2023 = MergedData[2]\n",
    "MergedBP2022 = MergedData[3]\n",
    "MergedBP2021 = MergedData[4]\n",
    "MergedBP2020 = MergedData[5]\n",
    "\n",
    "MergedBPall2025 = MergedDataAll[0]\n",
    "MergedBPall2024 = MergedDataAll[1]\n",
    "MergedBPall2023 = MergedDataAll[2]\n",
    "MergedBPall2022 = MergedDataAll[3]\n",
    "MergedBPall2021 = MergedDataAll[4]\n",
    "MergedBPall2020 = MergedDataAll[5]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data validation:",
   "id": "7bcef9027635fb9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#describe_dataframes(MergedData)\n",
    "\n",
    "describe_dataframes(MergedDataAll)"
   ],
   "id": "cce5c8e156072d1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interim data export:\n",
    "Notebook is getting too long and confusing, so I will export the interim data to csv files."
   ],
   "id": "e86d9f45953da6e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Exporting the interim data to csv files:\n",
    "\n",
    "save_df_list_to_csv_auto(MergedData, '../../data/interim/joined-balance-and-PnL/joined-only-matching-ja-kodas')\n",
    "\n",
    "save_df_list_to_csv_auto(MergedDataAll, '../../data/interim/joined-balance-and-PnL/joined-all-ja-kodas')"
   ],
   "id": "1457d0509ae9824e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
