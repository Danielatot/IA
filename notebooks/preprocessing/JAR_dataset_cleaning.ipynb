{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing Libraries and Source Functions",
   "id": "687104a251125851"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-15T14:17:59.284803Z",
     "start_time": "2025-11-15T14:17:58.918378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from typing import Union, List, Dict\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from src.python import check_and_pull_git_lfs, read_large_file\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..', 'src', 'python')))\n",
    "from Functions import *\n"
   ],
   "id": "855d7d0767c8db23",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the Dataset from CSV",
   "id": "b46b8ea3cc1649c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:18:10.657473Z",
     "start_time": "2025-11-15T14:18:01.411897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "check_and_pull_git_lfs() # Source function to check and pull git-lfs files\n",
    "\n",
    "# Load the dataset from CSV using source function\n",
    "balance2025 = read_large_file('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2025_n.csv')\n",
    "balance2024 = read_large_file('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2024_n.csv')\n",
    "balance2023 = read_large_file('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2023.csv')\n",
    "balance2022 = read_large_file('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2022.csv')\n",
    "balance2021 = read_large_file('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2021.csv')\n",
    "balance2020 = read_large_file('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2020.csv')\n",
    "\n",
    "pnl_2025 = read_large_file('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2025_n.csv')\n",
    "pnl_2024 = read_large_file('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2024_n.csv')\n",
    "pnl_2023 = read_large_file('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2023.csv')\n",
    "pnl_2022 = read_large_file('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2022.csv')\n",
    "pnl_2021 = read_large_file('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2021.csv')\n",
    "pnl_2020 = read_large_file('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2020.csv')\n"
   ],
   "id": "5483afd6c27b2c1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git repository root: /home/aidas/repos/Lithuanian-Innovation-Agency-Risk-Model\n",
      "Current working directory: /home/aidas/repos/Lithuanian-Innovation-Agency-Risk-Model/notebooks/preprocessing\n",
      "Note: Switching to repository root for LFS operations\n",
      "Git LFS version: git-lfs/3.7.1 (GitHub; linux amd64; go 1.25.1)\n",
      "Checking LFS status in repository root...\n",
      "LFS Status Output:\n",
      "On branch main\n",
      "Objects to be pushed to origin/main:\n",
      "\n",
      "\n",
      "Objects to be committed:\n",
      "\n",
      "\tdocs/TODO.txt (Git: e316ebc)\n",
      "\tnotebooks/preprocessing/JAR_dataset_cleaning.ipynb (Git: 99bc47f)\n",
      "\n",
      "Objects not staged for commit:\n",
      "\n",
      "\tnotebooks/preprocessing/Data-set-cleaning.ipynb (Git: 6539e89 -> File: f7619d5)\n",
      "\tnotebooks/preprocessing/Employment-data-cleaning.ipynb (Git: 63f529a -> File: 49c8ec0)\n",
      "\tnotebooks/preprocessing/JAR_dataset_cleaning.ipynb (File: e8a01bf)\n",
      "\tsrc/python/Functions.py (Git: 62718ad -> File: ff56442)\n",
      "\n",
      "\n",
      "No Git LFS files need downloading\n",
      "Reading CSV file from: ../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2025_n.csv\n",
      "File size: 176.02 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 1.125, '|': 16.0}\n",
      "Detected delimiter: '|' (score: 16.00)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 10000 rows (total: 120000)\n",
      "Processed chunk 13 with 10000 rows (total: 130000)\n",
      "Processed chunk 14 with 10000 rows (total: 140000)\n",
      "Processed chunk 15 with 10000 rows (total: 150000)\n",
      "Processed chunk 16 with 10000 rows (total: 160000)\n",
      "Processed chunk 17 with 10000 rows (total: 170000)\n",
      "Processed chunk 18 with 10000 rows (total: 180000)\n",
      "Processed chunk 19 with 10000 rows (total: 190000)\n",
      "Processed chunk 20 with 10000 rows (total: 200000)\n",
      "Processed chunk 21 with 10000 rows (total: 210000)\n",
      "Processed chunk 22 with 10000 rows (total: 220000)\n",
      "Processed chunk 23 with 10000 rows (total: 230000)\n",
      "Processed chunk 24 with 10000 rows (total: 240000)\n",
      "Processed chunk 25 with 10000 rows (total: 250000)\n",
      "Processed chunk 26 with 10000 rows (total: 260000)\n",
      "Processed chunk 27 with 10000 rows (total: 270000)\n",
      "Processed chunk 28 with 10000 rows (total: 280000)\n",
      "Processed chunk 29 with 10000 rows (total: 290000)\n",
      "Processed chunk 30 with 10000 rows (total: 300000)\n",
      "Processed chunk 31 with 10000 rows (total: 310000)\n",
      "Processed chunk 32 with 10000 rows (total: 320000)\n",
      "Processed chunk 33 with 10000 rows (total: 330000)\n",
      "Processed chunk 34 with 10000 rows (total: 340000)\n",
      "Processed chunk 35 with 10000 rows (total: 350000)\n",
      "Processed chunk 36 with 10000 rows (total: 360000)\n",
      "Processed chunk 37 with 10000 rows (total: 370000)\n",
      "Processed chunk 38 with 10000 rows (total: 380000)\n",
      "Processed chunk 39 with 10000 rows (total: 390000)\n",
      "Processed chunk 40 with 10000 rows (total: 400000)\n",
      "Processed chunk 41 with 10000 rows (total: 410000)\n",
      "Processed chunk 42 with 10000 rows (total: 420000)\n",
      "Processed chunk 43 with 10000 rows (total: 430000)\n",
      "Processed chunk 44 with 10000 rows (total: 440000)\n",
      "Processed chunk 45 with 10000 rows (total: 450000)\n",
      "Processed chunk 46 with 10000 rows (total: 460000)\n",
      "Processed chunk 47 with 10000 rows (total: 470000)\n",
      "Processed chunk 48 with 10000 rows (total: 480000)\n",
      "Processed chunk 49 with 10000 rows (total: 490000)\n",
      "Processed chunk 50 with 10000 rows (total: 500000)\n",
      "Processed chunk 51 with 10000 rows (total: 510000)\n",
      "Processed chunk 52 with 10000 rows (total: 520000)\n",
      "Processed chunk 53 with 10000 rows (total: 530000)\n",
      "Processed chunk 54 with 10000 rows (total: 540000)\n",
      "Processed chunk 55 with 10000 rows (total: 550000)\n",
      "Processed chunk 56 with 10000 rows (total: 560000)\n",
      "Processed chunk 57 with 10000 rows (total: 570000)\n",
      "Processed chunk 58 with 10000 rows (total: 580000)\n",
      "Processed chunk 59 with 10000 rows (total: 590000)\n",
      "Processed chunk 60 with 10000 rows (total: 600000)\n",
      "Processed chunk 61 with 10000 rows (total: 610000)\n",
      "Processed chunk 62 with 6861 rows (total: 616861)\n",
      "Concatenating 62 chunks...\n",
      "Finished reading. Total rows: 616861\n",
      "Reading CSV file from: ../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2024_n.csv\n",
      "File size: 172.70 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 1.125, '|': 16.0}\n",
      "Detected delimiter: '|' (score: 16.00)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 10000 rows (total: 120000)\n",
      "Processed chunk 13 with 10000 rows (total: 130000)\n",
      "Processed chunk 14 with 10000 rows (total: 140000)\n",
      "Processed chunk 15 with 10000 rows (total: 150000)\n",
      "Processed chunk 16 with 10000 rows (total: 160000)\n",
      "Processed chunk 17 with 10000 rows (total: 170000)\n",
      "Processed chunk 18 with 10000 rows (total: 180000)\n",
      "Processed chunk 19 with 10000 rows (total: 190000)\n",
      "Processed chunk 20 with 10000 rows (total: 200000)\n",
      "Processed chunk 21 with 10000 rows (total: 210000)\n",
      "Processed chunk 22 with 10000 rows (total: 220000)\n",
      "Processed chunk 23 with 10000 rows (total: 230000)\n",
      "Processed chunk 24 with 10000 rows (total: 240000)\n",
      "Processed chunk 25 with 10000 rows (total: 250000)\n",
      "Processed chunk 26 with 10000 rows (total: 260000)\n",
      "Processed chunk 27 with 10000 rows (total: 270000)\n",
      "Processed chunk 28 with 10000 rows (total: 280000)\n",
      "Processed chunk 29 with 10000 rows (total: 290000)\n",
      "Processed chunk 30 with 10000 rows (total: 300000)\n",
      "Processed chunk 31 with 10000 rows (total: 310000)\n",
      "Processed chunk 32 with 10000 rows (total: 320000)\n",
      "Processed chunk 33 with 10000 rows (total: 330000)\n",
      "Processed chunk 34 with 10000 rows (total: 340000)\n",
      "Processed chunk 35 with 10000 rows (total: 350000)\n",
      "Processed chunk 36 with 10000 rows (total: 360000)\n",
      "Processed chunk 37 with 10000 rows (total: 370000)\n",
      "Processed chunk 38 with 10000 rows (total: 380000)\n",
      "Processed chunk 39 with 10000 rows (total: 390000)\n",
      "Processed chunk 40 with 10000 rows (total: 400000)\n",
      "Processed chunk 41 with 10000 rows (total: 410000)\n",
      "Processed chunk 42 with 10000 rows (total: 420000)\n",
      "Processed chunk 43 with 10000 rows (total: 430000)\n",
      "Processed chunk 44 with 10000 rows (total: 440000)\n",
      "Processed chunk 45 with 10000 rows (total: 450000)\n",
      "Processed chunk 46 with 10000 rows (total: 460000)\n",
      "Processed chunk 47 with 10000 rows (total: 470000)\n",
      "Processed chunk 48 with 10000 rows (total: 480000)\n",
      "Processed chunk 49 with 10000 rows (total: 490000)\n",
      "Processed chunk 50 with 10000 rows (total: 500000)\n",
      "Processed chunk 51 with 10000 rows (total: 510000)\n",
      "Processed chunk 52 with 10000 rows (total: 520000)\n",
      "Processed chunk 53 with 10000 rows (total: 530000)\n",
      "Processed chunk 54 with 10000 rows (total: 540000)\n",
      "Processed chunk 55 with 10000 rows (total: 550000)\n",
      "Processed chunk 56 with 10000 rows (total: 560000)\n",
      "Processed chunk 57 with 10000 rows (total: 570000)\n",
      "Processed chunk 58 with 10000 rows (total: 580000)\n",
      "Processed chunk 59 with 10000 rows (total: 590000)\n",
      "Processed chunk 60 with 10000 rows (total: 600000)\n",
      "Processed chunk 61 with 4271 rows (total: 604271)\n",
      "Concatenating 61 chunks...\n",
      "Finished reading. Total rows: 604271\n",
      "Reading CSV file from: ../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2023.csv\n",
      "File size: 66.12 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 0.8450000000000001, '|': 15.0}\n",
      "Detected delimiter: '|' (score: 15.00)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 10000 rows (total: 120000)\n",
      "Processed chunk 13 with 10000 rows (total: 130000)\n",
      "Processed chunk 14 with 10000 rows (total: 140000)\n",
      "Processed chunk 15 with 10000 rows (total: 150000)\n",
      "Processed chunk 16 with 10000 rows (total: 160000)\n",
      "Processed chunk 17 with 10000 rows (total: 170000)\n",
      "Processed chunk 18 with 10000 rows (total: 180000)\n",
      "Processed chunk 19 with 10000 rows (total: 190000)\n",
      "Processed chunk 20 with 10000 rows (total: 200000)\n",
      "Processed chunk 21 with 10000 rows (total: 210000)\n",
      "Processed chunk 22 with 10000 rows (total: 220000)\n",
      "Processed chunk 23 with 10000 rows (total: 230000)\n",
      "Processed chunk 24 with 10000 rows (total: 240000)\n",
      "Processed chunk 25 with 10000 rows (total: 250000)\n",
      "Processed chunk 26 with 10000 rows (total: 260000)\n",
      "Processed chunk 27 with 10000 rows (total: 270000)\n",
      "Processed chunk 28 with 10000 rows (total: 280000)\n",
      "Processed chunk 29 with 10000 rows (total: 290000)\n",
      "Processed chunk 30 with 10000 rows (total: 300000)\n",
      "Processed chunk 31 with 4469 rows (total: 304469)\n",
      "Concatenating 31 chunks...\n",
      "Finished reading. Total rows: 304469\n",
      "Reading CSV file from: ../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2022.csv\n",
      "File size: 38.12 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 18.2}\n",
      "Detected delimiter: ',' (score: 18.20)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 10000 rows (total: 120000)\n",
      "Processed chunk 13 with 10000 rows (total: 130000)\n",
      "Processed chunk 14 with 10000 rows (total: 140000)\n",
      "Processed chunk 15 with 397 rows (total: 140397)\n",
      "Concatenating 15 chunks...\n",
      "Finished reading. Total rows: 140397\n",
      "Reading CSV file from: ../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2021.csv\n",
      "File size: 32.99 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 18.3}\n",
      "Detected delimiter: ',' (score: 18.30)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 10000 rows (total: 120000)\n",
      "Processed chunk 13 with 848 rows (total: 120848)\n",
      "Concatenating 13 chunks...\n",
      "Finished reading. Total rows: 120848\n",
      "Reading CSV file from: ../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2020.csv\n",
      "File size: 29.01 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 18.4}\n",
      "Detected delimiter: ',' (score: 18.40)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 6317 rows (total: 106317)\n",
      "Concatenating 11 chunks...\n",
      "Finished reading. Total rows: 106317\n",
      "Reading CSV file from: ../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2025_n.csv\n",
      "File size: 127.60 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 1.125, '|': 16.0}\n",
      "Detected delimiter: '|' (score: 16.00)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 10000 rows (total: 120000)\n",
      "Processed chunk 13 with 10000 rows (total: 130000)\n",
      "Processed chunk 14 with 10000 rows (total: 140000)\n",
      "Processed chunk 15 with 10000 rows (total: 150000)\n",
      "Processed chunk 16 with 10000 rows (total: 160000)\n",
      "Processed chunk 17 with 10000 rows (total: 170000)\n",
      "Processed chunk 18 with 10000 rows (total: 180000)\n",
      "Processed chunk 19 with 10000 rows (total: 190000)\n",
      "Processed chunk 20 with 10000 rows (total: 200000)\n",
      "Processed chunk 21 with 10000 rows (total: 210000)\n",
      "Processed chunk 22 with 10000 rows (total: 220000)\n",
      "Processed chunk 23 with 10000 rows (total: 230000)\n",
      "Processed chunk 24 with 10000 rows (total: 240000)\n",
      "Processed chunk 25 with 10000 rows (total: 250000)\n",
      "Processed chunk 26 with 10000 rows (total: 260000)\n",
      "Processed chunk 27 with 10000 rows (total: 270000)\n",
      "Processed chunk 28 with 10000 rows (total: 280000)\n",
      "Processed chunk 29 with 10000 rows (total: 290000)\n",
      "Processed chunk 30 with 10000 rows (total: 300000)\n",
      "Processed chunk 31 with 10000 rows (total: 310000)\n",
      "Processed chunk 32 with 10000 rows (total: 320000)\n",
      "Processed chunk 33 with 10000 rows (total: 330000)\n",
      "Processed chunk 34 with 10000 rows (total: 340000)\n",
      "Processed chunk 35 with 10000 rows (total: 350000)\n",
      "Processed chunk 36 with 10000 rows (total: 360000)\n",
      "Processed chunk 37 with 10000 rows (total: 370000)\n",
      "Processed chunk 38 with 10000 rows (total: 380000)\n",
      "Processed chunk 39 with 10000 rows (total: 390000)\n",
      "Processed chunk 40 with 10000 rows (total: 400000)\n",
      "Processed chunk 41 with 10000 rows (total: 410000)\n",
      "Processed chunk 42 with 10000 rows (total: 420000)\n",
      "Processed chunk 43 with 4552 rows (total: 424552)\n",
      "Concatenating 43 chunks...\n",
      "Finished reading. Total rows: 424552\n",
      "Reading CSV file from: ../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2024_n.csv\n",
      "File size: 122.53 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 1.125, '|': 16.0}\n",
      "Detected delimiter: '|' (score: 16.00)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 10000 rows (total: 120000)\n",
      "Processed chunk 13 with 10000 rows (total: 130000)\n",
      "Processed chunk 14 with 10000 rows (total: 140000)\n",
      "Processed chunk 15 with 10000 rows (total: 150000)\n",
      "Processed chunk 16 with 10000 rows (total: 160000)\n",
      "Processed chunk 17 with 10000 rows (total: 170000)\n",
      "Processed chunk 18 with 10000 rows (total: 180000)\n",
      "Processed chunk 19 with 10000 rows (total: 190000)\n",
      "Processed chunk 20 with 10000 rows (total: 200000)\n",
      "Processed chunk 21 with 10000 rows (total: 210000)\n",
      "Processed chunk 22 with 10000 rows (total: 220000)\n",
      "Processed chunk 23 with 10000 rows (total: 230000)\n",
      "Processed chunk 24 with 10000 rows (total: 240000)\n",
      "Processed chunk 25 with 10000 rows (total: 250000)\n",
      "Processed chunk 26 with 10000 rows (total: 260000)\n",
      "Processed chunk 27 with 10000 rows (total: 270000)\n",
      "Processed chunk 28 with 10000 rows (total: 280000)\n",
      "Processed chunk 29 with 10000 rows (total: 290000)\n",
      "Processed chunk 30 with 10000 rows (total: 300000)\n",
      "Processed chunk 31 with 10000 rows (total: 310000)\n",
      "Processed chunk 32 with 10000 rows (total: 320000)\n",
      "Processed chunk 33 with 10000 rows (total: 330000)\n",
      "Processed chunk 34 with 10000 rows (total: 340000)\n",
      "Processed chunk 35 with 10000 rows (total: 350000)\n",
      "Processed chunk 36 with 10000 rows (total: 360000)\n",
      "Processed chunk 37 with 10000 rows (total: 370000)\n",
      "Processed chunk 38 with 10000 rows (total: 380000)\n",
      "Processed chunk 39 with 10000 rows (total: 390000)\n",
      "Processed chunk 40 with 10000 rows (total: 400000)\n",
      "Processed chunk 41 with 6755 rows (total: 406755)\n",
      "Concatenating 41 chunks...\n",
      "Finished reading. Total rows: 406755\n",
      "Reading CSV file from: ../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2023.csv\n",
      "File size: 39.43 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 1.2800000000000002, '|': 16.0}\n",
      "Detected delimiter: '|' (score: 16.00)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 10000 rows (total: 120000)\n",
      "Processed chunk 13 with 10000 rows (total: 130000)\n",
      "Processed chunk 14 with 10000 rows (total: 140000)\n",
      "Processed chunk 15 with 1944 rows (total: 141944)\n",
      "Concatenating 15 chunks...\n",
      "Finished reading. Total rows: 141944\n",
      "Reading CSV file from: ../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2022.csv\n",
      "File size: 34.65 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 17.4}\n",
      "Detected delimiter: ',' (score: 17.40)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 10000 rows (total: 120000)\n",
      "Processed chunk 13 with 3690 rows (total: 123690)\n",
      "Concatenating 13 chunks...\n",
      "Finished reading. Total rows: 123690\n",
      "Reading CSV file from: ../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2021.csv\n",
      "File size: 30.90 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 17.4}\n",
      "Detected delimiter: ',' (score: 17.40)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 10000 rows (total: 100000)\n",
      "Processed chunk 11 with 10000 rows (total: 110000)\n",
      "Processed chunk 12 with 225 rows (total: 110225)\n",
      "Concatenating 12 chunks...\n",
      "Finished reading. Total rows: 110225\n",
      "Reading CSV file from: ../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2020.csv\n",
      "File size: 27.05 MB\n",
      "Auto-detecting CSV delimiter...\n",
      "Delimiter detection scores: {',': 17.6}\n",
      "Detected delimiter: ',' (score: 17.60)\n",
      "Bad lines handling: error\n",
      "File encoding: utf-8\n",
      "Processed chunk 1 with 10000 rows (total: 10000)\n",
      "Processed chunk 2 with 10000 rows (total: 20000)\n",
      "Processed chunk 3 with 10000 rows (total: 30000)\n",
      "Processed chunk 4 with 10000 rows (total: 40000)\n",
      "Processed chunk 5 with 10000 rows (total: 50000)\n",
      "Processed chunk 6 with 10000 rows (total: 60000)\n",
      "Processed chunk 7 with 10000 rows (total: 70000)\n",
      "Processed chunk 8 with 10000 rows (total: 80000)\n",
      "Processed chunk 9 with 10000 rows (total: 90000)\n",
      "Processed chunk 10 with 6615 rows (total: 96615)\n",
      "Concatenating 10 chunks...\n",
      "Finished reading. Total rows: 96615\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Making backups of the original datasets",
   "id": "752003cdd817b7a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:20:23.031280Z",
     "start_time": "2025-11-15T14:20:22.368002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Balance statements\n",
    "B2025 = balance2025.copy()\n",
    "B2024 = balance2024.copy()\n",
    "B2023 = balance2023.copy()\n",
    "B2022 = balance2022.copy()\n",
    "B2021 = balance2021.copy()\n",
    "B2020 = balance2020.copy()\n",
    "\n",
    "# PnL statements\n",
    "P2025 = pnl_2025.copy()\n",
    "P2024 = pnl_2024.copy()\n",
    "P2023 = pnl_2023.copy()\n",
    "P2022 = pnl_2022.copy()\n",
    "P2021 = pnl_2021.copy()\n",
    "P2020 = pnl_2020.copy()\n",
    "\n",
    "# Saving into list for further processing\n",
    "balance_statements = [B2025, B2024, B2023, B2022, B2021, B2020]\n",
    "pnl_statements = [P2025, P2024, P2023, P2022, P2021, P2020]"
   ],
   "id": "be8b0b0c96bb298e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Removing unnecessary columns",
   "id": "d365bf944329afcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T14:33:53.032252Z",
     "start_time": "2025-11-15T14:33:52.935969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Balance statements\n",
    "\n",
    "bad_cols_bal = ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id' ]\n",
    "\n",
    "balance_statements = remove_columns(balance_statements, bad_cols_bal, verbose=True, inplace=True)\n",
    "\n",
    "# PnL statements\n",
    "\n",
    "bad_cols_pnl = ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id' ]\n",
    "\n",
    "pnl_statements = remove_columns(pnl_statements, bad_cols_pnl, verbose=True, inplace=True)"
   ],
   "id": "79b0899a4d6434cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DataFrame 0:\n",
      "   ‚è≠Ô∏è  Skipped: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 10\n",
      "üìä DataFrame 1:\n",
      "   ‚è≠Ô∏è  Skipped: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 10\n",
      "üìä DataFrame 2:\n",
      "   ‚è≠Ô∏è  Skipped: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 12\n",
      "üìä DataFrame 3:\n",
      "   ‚è≠Ô∏è  Skipped: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 12\n",
      "üìä DataFrame 4:\n",
      "   ‚è≠Ô∏è  Skipped: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 12\n",
      "üìä DataFrame 5:\n",
      "   ‚è≠Ô∏è  Skipped: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 12\n",
      "\n",
      "üéØ FINAL SUMMARY:\n",
      "   üì¶ Processed 6 DataFrames\n",
      "   üóëÔ∏è  Total columns removed: 0\n",
      "   ‚è≠Ô∏è  Total columns skipped: 0\n",
      "üìä DataFrame 0:\n",
      "   ‚úÖ Removed: ['template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   ‚è≠Ô∏è  Skipped: ['obj_pav', 'form_pav', 'stat_pav']\n",
      "   üìã Remaining columns: 10\n",
      "üìä DataFrame 1:\n",
      "   ‚úÖ Removed: ['template_name', 'standard_name', 'formavimo_data', 'ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   ‚è≠Ô∏è  Skipped: ['obj_pav', 'form_pav', 'stat_pav']\n",
      "   üìã Remaining columns: 10\n",
      "üìä DataFrame 2:\n",
      "   ‚úÖ Removed: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data']\n",
      "   ‚è≠Ô∏è  Skipped: ['ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 11\n",
      "üìä DataFrame 3:\n",
      "   ‚úÖ Removed: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data']\n",
      "   ‚è≠Ô∏è  Skipped: ['ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 11\n",
      "üìä DataFrame 4:\n",
      "   ‚úÖ Removed: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data']\n",
      "   ‚è≠Ô∏è  Skipped: ['ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 11\n",
      "üìä DataFrame 5:\n",
      "   ‚úÖ Removed: ['obj_pav', 'form_pav', 'stat_pav', 'template_name', 'standard_name', 'formavimo_data']\n",
      "   ‚è≠Ô∏è  Skipped: ['ja_pavadinimas', 'form_pavadinimas', 'stat_pavadinimas', 'line_type_id']\n",
      "   üìã Remaining columns: 11\n",
      "\n",
      "üéØ FINAL SUMMARY:\n",
      "   üì¶ Processed 6 DataFrames\n",
      "   üóëÔ∏è  Total columns removed: 38\n",
      "   ‚è≠Ô∏è  Total columns skipped: 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Renaming columns to be the same across all datasets",
   "id": "b056fc413bb4cd85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Balance statements\n",
    "\n",
    "rename"
   ],
   "id": "371392059a924c84"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
